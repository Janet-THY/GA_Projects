{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c93fe72-d917-4076-adb9-67dc22703c57",
   "metadata": {},
   "source": [
    "# Sentiment and Emotion Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a7eb51-769b-4076-bb69-fd48d4fe9354",
   "metadata": {},
   "source": [
    "Notebook 4 of 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1cc045-8c17-4b1d-a5c1-bfd2f87f77b1",
   "metadata": {},
   "source": [
    "We will analyse the post from each of the subreddits as well as some of the major topics from each to get understanding on the communities sentiment and emotion. In order to do so, we will utilise the Hugging Face pre-trained models for sentiment analysis as well as emotion analysis.\n",
    " \n",
    "The topics for each subreddit we will explore are:\n",
    "- Dunkin Donuts\n",
    " 1. dunkin donuts \n",
    " 2. cold brew vs cold foam\n",
    " 3. iced coffee vs frozen coffee \n",
    " 4. butter pecan\n",
    " 5. local dunkin\n",
    " \n",
    "- Starbucks\n",
    " 1. dress code? \n",
    " 2. pumpkin spice\n",
    " 3. cold brew vs cold foam\n",
    " 4. apple crisp\n",
    " 5. fall launch\n",
    " \n",
    "Dunkin Donuts and Starbucks are the brands name? These follows by the top 3 most popular products for each subreddit based on the frequency of the words appear in the subreddit. The local and upcoming launch of product are also hot topics in both subreddits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064b9ad-725e-463a-927e-718fae92f29e",
   "metadata": {},
   "source": [
    "## Import Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a604592e-2077-417f-8083-c3ea3e06f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dea8d91-8dcc-410c-91f1-d33293837c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4623, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.read_csv('./datasets/combined_cleaned_reddit_selftext.csv')\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94c99b62-e1f1-47c7-9285-bdd771e27cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4623 entries, 0 to 4996\n",
      "Data columns (total 4 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   subreddit               4623 non-null   int64 \n",
      " 1   title_selftext          4623 non-null   object\n",
      " 2   created_utc             4623 non-null   int64 \n",
      " 3   stemmed_title_selftext  4623 non-null   object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 180.6+ KB\n"
     ]
    }
   ],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f57d1e-093c-4014-a7d7-642dfd00c229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>stemmed_title_selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>cowork place hash brown like armi troopsfacewi...</td>\n",
       "      <td>1663204910</td>\n",
       "      <td>cowork place hash brown like armi troopsfacewi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>make ice tea order door dash tast ice tea orde...</td>\n",
       "      <td>1663190691</td>\n",
       "      <td>make ice tea order door dash tast ice tea orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>still got hour shift gotexplodinghead,nan</td>\n",
       "      <td>1663185603</td>\n",
       "      <td>still got hour shift gotexplodinghead</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                     title_selftext  created_utc  \\\n",
       "0          1  cowork place hash brown like armi troopsfacewi...   1663204910   \n",
       "3          1  make ice tea order door dash tast ice tea orde...   1663190691   \n",
       "4          1          still got hour shift gotexplodinghead,nan   1663185603   \n",
       "\n",
       "                              stemmed_title_selftext  \n",
       "0  cowork place hash brown like armi troopsfacewi...  \n",
       "3  make ice tea order door dash tast ice tea orde...  \n",
       "4              still got hour shift gotexplodinghead  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a529927a-8d1a-4c08-aa98-b8d82105559a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit                 0\n",
       "title_selftext            0\n",
       "created_utc               0\n",
       "stemmed_title_selftext    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values\n",
    "combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6eb31b-316d-4c19-b4c4-2eaa51460e66",
   "metadata": {},
   "source": [
    "There is no missing values in datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab464b8-8805-4178-82a9-0923a25b88ba",
   "metadata": {},
   "source": [
    "### Tokenize words and join back into a sentence to remove unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb0da46b-6056-477a-9968-a1d0958581fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac4b5e41-e867-4cd8-947d-a9f7a6731c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>stemmed_title_selftext</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>cowork place hash brown like armi troopsfacewi...</td>\n",
       "      <td>1663204910</td>\n",
       "      <td>cowork place hash brown like armi troopsfacewi...</td>\n",
       "      <td>[cowork, place, hash, brown, like, armi, troop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>make ice tea order door dash tast ice tea orde...</td>\n",
       "      <td>1663190691</td>\n",
       "      <td>make ice tea order door dash tast ice tea orde...</td>\n",
       "      <td>[make, ice, tea, order, door, dash, tast, ice,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>still got hour shift gotexplodinghead,nan</td>\n",
       "      <td>1663185603</td>\n",
       "      <td>still got hour shift gotexplodinghead</td>\n",
       "      <td>[still, got, hour, shift, gotexplodinghead, nan]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                     title_selftext  created_utc  \\\n",
       "0          1  cowork place hash brown like armi troopsfacewi...   1663204910   \n",
       "3          1  make ice tea order door dash tast ice tea orde...   1663190691   \n",
       "4          1          still got hour shift gotexplodinghead,nan   1663185603   \n",
       "\n",
       "                              stemmed_title_selftext  \\\n",
       "0  cowork place hash brown like armi troopsfacewi...   \n",
       "3  make ice tea order door dash tast ice tea orde...   \n",
       "4              still got hour shift gotexplodinghead   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [cowork, place, hash, brown, like, armi, troop...  \n",
       "3  [make, ice, tea, order, door, dash, tast, ice,...  \n",
       "4   [still, got, hour, shift, gotexplodinghead, nan]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['tokenized'] = combined_df['title_selftext'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "combined_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8e3de39-b837-4611-b360-4c390745ee04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>stemmed_title_selftext</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>cowork place hash brown like armi troopsfacewi...</td>\n",
       "      <td>1663204910</td>\n",
       "      <td>cowork place hash brown like armi troopsfacewi...</td>\n",
       "      <td>[cowork, place, hash, brown, like, armi, troop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>make ice tea order door dash tast ice tea orde...</td>\n",
       "      <td>1663190691</td>\n",
       "      <td>make ice tea order door dash tast ice tea orde...</td>\n",
       "      <td>[make, ice, tea, order, door, dash, tast, ice,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>still got hour shift gotexplodinghead nan</td>\n",
       "      <td>1663185603</td>\n",
       "      <td>still got hour shift gotexplodinghead</td>\n",
       "      <td>[still, got, hour, shift, gotexplodinghead, nan]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                     title_selftext  created_utc  \\\n",
       "0          1  cowork place hash brown like armi troopsfacewi...   1663204910   \n",
       "3          1  make ice tea order door dash tast ice tea orde...   1663190691   \n",
       "4          1          still got hour shift gotexplodinghead nan   1663185603   \n",
       "\n",
       "                              stemmed_title_selftext  \\\n",
       "0  cowork place hash brown like armi troopsfacewi...   \n",
       "3  make ice tea order door dash tast ice tea orde...   \n",
       "4              still got hour shift gotexplodinghead   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [cowork, place, hash, brown, like, armi, troop...  \n",
       "3  [make, ice, tea, order, door, dash, tast, ice,...  \n",
       "4   [still, got, hour, shift, gotexplodinghead, nan]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['title_selftext'] = combined_df['tokenized'].apply(lambda x: \" \".join(x))\n",
    "combined_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cead8e-7279-4e78-95a7-b272a084bb6c",
   "metadata": {},
   "source": [
    "### Separate into Starbucks and Dunkin Donuts datasets for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bea73e9-10f4-4ba9-90df-3fc3ab7cbd05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2306, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddonuts_text_df = combined_df[combined_df['subreddit'] == 1]\n",
    "ddonuts_text_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efa6f4ff-3479-4a63-81d3-6c5a6ee14d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>stemmed_title_selftext</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>cowork place hash brown like armi troopsfacewi...</td>\n",
       "      <td>1663204910</td>\n",
       "      <td>cowork place hash brown like armi troopsfacewi...</td>\n",
       "      <td>[cowork, place, hash, brown, like, armi, troop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>make ice tea order door dash tast ice tea orde...</td>\n",
       "      <td>1663190691</td>\n",
       "      <td>make ice tea order door dash tast ice tea orde...</td>\n",
       "      <td>[make, ice, tea, order, door, dash, tast, ice,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>still got hour shift gotexplodinghead nan</td>\n",
       "      <td>1663185603</td>\n",
       "      <td>still got hour shift gotexplodinghead</td>\n",
       "      <td>[still, got, hour, shift, gotexplodinghead, nan]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                     title_selftext  created_utc  \\\n",
       "0          1  cowork place hash brown like armi troopsfacewi...   1663204910   \n",
       "3          1  make ice tea order door dash tast ice tea orde...   1663190691   \n",
       "4          1          still got hour shift gotexplodinghead nan   1663185603   \n",
       "\n",
       "                              stemmed_title_selftext  \\\n",
       "0  cowork place hash brown like armi troopsfacewi...   \n",
       "3  make ice tea order door dash tast ice tea orde...   \n",
       "4              still got hour shift gotexplodinghead   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [cowork, place, hash, brown, like, armi, troop...  \n",
       "3  [make, ice, tea, order, door, dash, tast, ice,...  \n",
       "4   [still, got, hour, shift, gotexplodinghead, nan]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddonuts_text_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc09f3f7-96f7-4e59-b267-ed3a9724f9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2317, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbucks_text_df = combined_df[combined_df['subreddit'] == 0]\n",
    "sbucks_text_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88229c88-ec64-47cd-affb-129d07895db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>stemmed_title_selftext</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>0</td>\n",
       "      <td>interview tip hi hope question isn t repetitiv...</td>\n",
       "      <td>1663212467</td>\n",
       "      <td>interview tip hi hope question repetitiveannoy...</td>\n",
       "      <td>[interview, tip, hi, hope, question, isn, t, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>0</td>\n",
       "      <td>hors come drivethru recent present caffein cav...</td>\n",
       "      <td>1663212017</td>\n",
       "      <td>hor come drivethru recent present caffein cavalri</td>\n",
       "      <td>[hors, come, drivethru, recent, present, caffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>0</td>\n",
       "      <td>hors drivethru make everyth better present caf...</td>\n",
       "      <td>1663211903</td>\n",
       "      <td>hor drivethru make everyth better present caff...</td>\n",
       "      <td>[hors, drivethru, make, everyth, better, prese...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit                                     title_selftext  \\\n",
       "2498          0  interview tip hi hope question isn t repetitiv...   \n",
       "2499          0  hors come drivethru recent present caffein cav...   \n",
       "2500          0  hors drivethru make everyth better present caf...   \n",
       "\n",
       "      created_utc                             stemmed_title_selftext  \\\n",
       "2498   1663212467  interview tip hi hope question repetitiveannoy...   \n",
       "2499   1663212017  hor come drivethru recent present caffein cavalri   \n",
       "2500   1663211903  hor drivethru make everyth better present caff...   \n",
       "\n",
       "                                              tokenized  \n",
       "2498  [interview, tip, hi, hope, question, isn, t, r...  \n",
       "2499  [hors, come, drivethru, recent, present, caffe...  \n",
       "2500  [hors, drivethru, make, everyth, better, prese...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbucks_text_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e733be0-b04b-4b53-9884-3e3b6223e3b2",
   "metadata": {},
   "source": [
    "### Create separate dataframe for each of the subtopics for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81f8f2bc-1498-442f-b2e5-e8a373a4decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dunkin_donuts = ddonuts_text_df[ddonuts_text_df['title_selftext'].str.contains('dunkin donuts')]\n",
    "dunkin_c_brew = ddonuts_text_df[ddonuts_text_df['title_selftext'].str.contains('cold brew')]\n",
    "dunkin_i_coffee = ddonuts_text_df[ddonuts_text_df['title_selftext'].str.contains('iced coffee')]\n",
    "dunkin_b_pecan = ddonuts_text_df[ddonuts_text_df['title_selftext'].str.contains('butter pecan')]\n",
    "dunkin_local = ddonuts_text_df[ddonuts_text_df['title_selftext'].str.contains('local dunkin')]\n",
    "\n",
    "sbucks_dress = sbucks_text_df[sbucks_text_df['title_selftext'].str.contains('dress code')]\n",
    "sbucks_p_spice = sbucks_text_df[sbucks_text_df['title_selftext'].str.contains('pumkin spice')]\n",
    "sbucks_c_brew = sbucks_text_df[sbucks_text_df['title_selftext'].str.contains('cold brew')]\n",
    "sbucks_a_crisp = sbucks_text_df[sbucks_text_df['title_selftext'].str.contains('apple crisp')]\n",
    "sbucks_f_launch = sbucks_text_df[sbucks_text_df['title_selftext'].str.contains('fall_launch')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5419e62-35dd-4932-90c7-b6ae2caae7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dunkin_donuts = dunkin_donuts.copy()\n",
    "dunkin_c_brew = dunkin_c_brew.copy()\n",
    "dunkin_i_coffee = dunkin_i_coffee.copy()\n",
    "dunkin_b_pecan = dunkin_b_pecan.copy()\n",
    "dunkin_local = dunkin_local.copy()\n",
    "\n",
    "sbucks_dress = sbucks_text_df[sbucks_text_df['title_selftext'].str.contains('dress code')]\n",
    "sbucks_p_spice = sbucks_text_df[sbucks_text_df['title_selftext'].str.contains('pumkin spice')]\n",
    "sbucks_c_brew = sbucks_text_df[sbucks_text_df['title_selftext'].str.contains('cold brew')]\n",
    "sbucks_a_crisp = sbucks_text_df[sbucks_text_df['title_selftext'].str.contains('apple crisp')]\n",
    "sbucks_f_launch = sbucks_text_df[sbucks_text_df['title_selftext'].str.contains('fall_launch')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36321b-b481-4052-b008-bedeeb7d21dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
